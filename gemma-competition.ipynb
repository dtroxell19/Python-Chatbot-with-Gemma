{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7114700,"sourceType":"datasetVersion","datasetId":4102922},{"sourceId":7691577,"sourceType":"datasetVersion","datasetId":4488812},{"sourceId":7790610,"sourceType":"datasetVersion","datasetId":4560179},{"sourceId":7790632,"sourceType":"datasetVersion","datasetId":4560196},{"sourceId":7856134,"sourceType":"datasetVersion","datasetId":4607812},{"sourceId":7925822,"sourceType":"datasetVersion","datasetId":4658074},{"sourceId":8068309,"sourceType":"datasetVersion","datasetId":4760375},{"sourceId":8070373,"sourceType":"datasetVersion","datasetId":4761820},{"sourceId":11372,"sourceType":"modelInstanceVersion","modelInstanceId":5388}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<span style='margin-left: 100px;'><h1>Introducing PyGEM: Submission for [Google - AI Assistants for Data Tasks with Gemma](https://www.kaggle.com/competitions/data-assistants-with-gemma) Kaggle Competition\n    \n<span style='margin-left: 300px;'>![](https://i.imgur.com/CQQb5NH.png)\n    \n# What makes this submission unique?\n1. A user-friendly and interactive way to ask questions to the chatbot\n2. Quantifiable metrics beyond just human judgement of queries\n3. Novel datasets used to train the model (in addition to other data sources)\n4. Both RAG and fine-tuning demonstrated and implemented\n    \n# Description\nThis notebook showcases how Google's Gemma models can easily be used to create cost-effective and high-quality language models. A chatbot named \"PyGEM\" (a mix of \"Python\" and \"Gemma\") is designed to answer questions about the Python programming language. Along the way, the concepts of Retrieval-Augmented Generation (RAG), embedding models, large language model evaluation metrics, model fine-tuning with low-rank adaptation (LoRA), and interactive displays are discussed. Enjoy!\n    \n# Table of contents\n1. [Retrieval-Augmented Generation (RAG)](#RAG)\n    1. [Data Descriptions and Document Creation](#Create)\n    2. [Embedding Model and Knowledge Base Construction](#Construct)\n    3. [Context Retrieval and Prompt Engineering](#Engineer)\n2. [Model Creation and Enabling Low Rank Adaptation (LoRA)](#Model)\n3. [Evaluation Metrics before Fine-Tuning](#Metrics)\n4. [Model Fine-Tuning and Re-Evaluation](#FT)\n5. [Interactive Messaging and Displays](#UI)\n6. [Conclusion and Limitations](#Conclusion)\n7. [Screenshots of Notebook Output](#results)\n    \nTo go straight to the final model with RAG and the function to ask the model questions (without interface, explanation of code, or calculation of evaluation metrics as done below), [click here](#MVP).\n","metadata":{}},{"cell_type":"code","source":"#Install and import all required packages for the notebook\n\n# !pip install rouge-score\n# !pip install langchain langchain-openai faiss-cpu tiktoken\n# !pip install chromadb\n# !pip install sentence-transformers\n# !pip install PyPDF2\n# !pip install ipywidgets\n# !jupyter nbextension enable --py --sys-prefix widgetsnbextension\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport os\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.schema import Document\nfrom langchain.vectorstores.chroma import Chroma\nfrom PyPDF2 import PdfReader\nimport keras\nimport keras_nlp\nimport tensorflow as tf\nfrom rouge_score import rouge_scorer\nfrom langchain.embeddings import SentenceTransformerEmbeddings\nimport time\nimport matplotlib.pyplot as plt\nfrom IPython.display import display, HTML\nimport ipywidgets as widgets\nimport sentencepiece\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n#define the backend for model training\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\n#set to high fraction to help prevent memory fragmentation\nos.environ['XLA_PYTHON_CLIENT_MEM_FRACTION']='1.00'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Retrieval-Augmented Generation (RAG) <a name=\"RAG\"></a>","metadata":{}},{"cell_type":"markdown","source":"[Retrieval-Augmented Generation (RAG)](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/) is one techinque used to help prevent LLMs from hallucinating nonsense answers to questions a user may have. A RAG system may roughly be broken down into a few steps:\n\n1. [**Document Gathering and Cleaning**](#Create) - Various potentially relevant documents to the task are gathered and then split into chunks of text.\n2. [**Embedding Model and Knowledge Base Construction**](#Construct) - A (typically pre-trained) embedding model is selected that transforms the chunked text into [word embeddings](https://www.tensorflow.org/text/tutorials/word_embeddings). These embeddings are stored in a vector database.\n3. [**Context Retrieval and Prompt Engineering**](#Engineer) - A query is embedded and then compared to all embeddings of chunked text in the knowledge base. The top k chunks are then fed to the model for additional context before answering a question. \n\nEach of the 3 steps are performed below.","metadata":{}},{"cell_type":"markdown","source":"## 1. Data Descriptions + Document Gathering and Cleaning<a name=\"Create\"></a>","metadata":{}},{"cell_type":"code","source":"#This notebook cell defines various helper functions that we will use in the RAG system construction\n\ndef create_document(pdf_path,start_idx=None, end_idx=None):\n    '''\n    Extracts text from a pdf and Creates a Langchain Document object\n\n    Parameters:\n    pdf_path (str): File path of the pdf to be placed into knowledge base\n    start_idx (int): Deletes the first n characters of the document if irrelavent\n    end_idx (int): Deletes the last n characters of the document if irrelavent\n\n    Returns:\n    doc (Document): The pdf text as a Langchain Document Object\n    '''\n    text = ''\n    #extract the text from the pdf\n    with open(pdf_path, 'rb') as file:\n        reader = PdfReader(file)\n        for page in reader.pages:\n            text += page.extract_text()\n            \n    #if we know when relevant info for the doc starts and ends, cut off irrelevant parts\n    if start_idx and end_idx:\n        text=text[start_idx:end_idx]\n    \n    #create the document and record where the text came from\n    doc = Document(page_content=text,metadata={\"source\":pdf_path})\n      \n    return doc\n\ndef split_text(documents: list[Document]):\n    '''\n    Converts Langchain Document objects into many smaller Document objects \n    of chunked text\n\n    Parameters:\n    documents (list[Document]): list of Langchain Document objects to be chunked\n\n    Returns:\n    chunks (list[Document]): chunked text as Langchain Document objects\n    '''\n    \n    #define hyperparameters for how to split text into chunks\n    text_splitter = RecursiveCharacterTextSplitter(\n        #number of characters per chunk\n        chunk_size=300,\n        #make chunks overlap to hopefully capture important info\n        chunk_overlap=100,\n        length_function=len,\n        add_start_index=True,\n    )\n    chunks = text_splitter.split_documents(documents)\n    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n\n\n    return chunks\n\ndef split_df_RAG(df,path,RAG_proportion=.5):\n    '''\n    To reduce training time, can place some of training set into RAG system\n    and exclude from training. This function randomly transforms some\n    observations into RAG Documents.\n\n    Parameters:\n    df (pd.DataFrame): dataFrame with <QUESTION> column and <RESPONSE> column\n    path (str): location of data path for data in df\n    RAG_proportion (float): percentage of observations to transfer to RAG\n\n    Returns:\n    doc (Langchain Document): Un-chunked Document for RAG\n    df (pd.DataFrame): remaining observations to keep for training set\n    '''\n    \n    #get random indices to keep for RAG\n    random_indices = np.random.permutation(df.shape[0])\n    RAG_idx = random_indices[:int(df.shape[0]*RAG_proportion)]\n    #get corresponding training set indices\n    remaining_idx = random_indices[int(df.shape[0]*RAG_proportion):]\n    #convert RAG examples into a document\n    RAG_examples = df.iloc[RAG_idx]\n    RAG_text = (RAG_examples['<QUESTION>']+' \\n' +RAG_examples['<RESPONSE>']).values\n    RAG_text = ' '.join(RAG_text)\n    doc = Document(page_content=RAG_text,metadata={\"source\":path})\n    #remove the RAG examples from the training set\n    df = df.iloc[remaining_idx]\n    \n    return doc,df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Data Sources:\n1. python-code-questions.jsonl: novel dataset consisting of basic python code tasks and appropriate responses. Gathered using a combination of human-reviewed and AI-generated questions and responses.\n\n2. Python_Conceptual_Questions.jsonl: novel dataset consisting of basic general python questions (with no code) and appropriate responses. Gathered using a combination of human-reviewed and AI-generated questions and responses.\n\n3. python-codes-25k.jsonl (source: https://huggingface.co/datasets/flytech/python-codes-25k): database of instructions revolving around daily tasks and corresponding python code to generate a basic function to complete such tasks.\n\n4. mbpp.jsonl (source: https://www.kaggle.com/datasets/mpwolke/mbppjsonl/data): crowdsourced database of basic python code questions and corresponding responses.","metadata":{}},{"cell_type":"code","source":"#For each of our 4 data sources, load the data and send 90% of it to the RAG system.\n#By doing this, we can 1) still allow the model to access information regarding a vast\n#amount of common questions that hopefully span a diverse set of prompts 2) speed up\n#training-time 3) still have a large enough training set to craft response style and\n#hopefully become better at answering questions outside of knowledge base\n\n#first two data sources already just have <QUESTION> and <RESPONSE> columns\nnp.random.seed(1)\ndf1_path = '/kaggle/input/python-code-questions/python_code_questions.jsonl'\ndf1 = pd.read_json(df1_path, lines=True)\nRAG1, df1 = split_df_RAG(df1,df1_path,.9)\n\ndf2_path = '/kaggle/input/python-conceptual-q/Python_Conceptual_Questions.jsonl'\ndf2 = pd.read_json(df2_path, lines=True)\nRAG2, df2 = split_df_RAG(df2,df2_path,.9)\n\n#last two datasets need same columns as first two\ndf3_path = '/kaggle/input/flytech-pythoncodes-25k-dataset/python-codes-25k.jsonl'\ndf3 = pd.read_json(df3_path, lines=True)\ndf3=df3[['instruction','output']]\ndf3.rename(columns={'instruction': '<QUESTION>', 'output': '<RESPONSE>'}, inplace=True)\nRAG3, df3 = split_df_RAG(df3,df3_path,.9)\n\ndf4_path = '/kaggle/input/mbppjsonl/mbpp.jsonl'\ndf4 = pd.read_json(df4_path, lines=True)\ndf4=df4[['text','code']]\ndf4.rename(columns={'text': '<QUESTION>', 'code': '<RESPONSE>'}, inplace=True)\nRAG4, df4 = split_df_RAG(df4,df4_path,.9)\n\n#combine into one dataset\ndf = pd.concat([df1, df2, df3, df4], ignore_index=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Textbook Sources for Knowledge Base:\n1. **Introduction to Machine Learning with Python** - Andreas C. MÃ¼ller & Sarah Guido. Source: https://www.nrigroupindia.com/e-book/Introduction%20to%20Machine%20Learning%20with%20Python%20(%20PDFDrive.com%20)-min.pdf\n\n2. **Learning Python** - Mark Lutz. Source: https://cfm.ehu.es/ricardo/docs/python/Learning_Python.pdf\n\n3. **Programming Python** - Mark Lutz. Source: http://bilal-qudah.com/mm/Programming%20Python%20Fourth%20Edition.pdf","metadata":{}},{"cell_type":"code","source":"#Load Documents for Knowledge Base\ndocuments=[RAG1,RAG2,RAG3,RAG4]\n\nstart_time = time.time()\nfor dirname, _, filenames in os.walk('/kaggle/input/knowledge-base'):\n    counter=0\n    for filename in filenames:\n        counter+=1\n        print(\"Loading textbook number {}\".format(counter))\n        doc_path = os.path.join(dirname, filename)\n        documents.append(create_document(doc_path))\n\n#Split document into multiple objects\nchunks = split_text(documents)\nend_time = time.time()\nprint(\"\\nElapsed time:\", end_time-start_time, \"seconds\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Embedding Model and Knowledge Base Construction <a name=\"Construct\"></a>\nThe [GIST embedding models](https://arxiv.org/abs/2402.16829) are a project funded by the Word Bank. The model class performs well on the [MTEB (Massive Text Embedding Benchmark) leaderboard](https://huggingface.co/spaces/mteb/leaderboard), yet are low in computational memory requirements, and are thus used in this notebook. The embedding model employed in this notebook is the \"small\" version within the model class. At the time of notebook model construction, the small version of GIST ranked 22nd on the leaderboard which measures embedding models on a wide array of tasks. ","metadata":{}},{"cell_type":"code","source":"#Use embedding model from HF \nembeddings = SentenceTransformerEmbeddings(model_name = \"avsolatorio/GIST-small-Embedding-v0\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #Create vector database to hold chunked text\n\n# start_time = time.time()\n# #Using Chroma db\n# vectordb = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory='./vectordb')\n# #When running in notebook, need to use .persist()\n# vectordb.persist() \n# end_time = time.time()\n# print(\"\\nElapsed time:\", end_time-start_time, \"seconds\")\n\n# #save database for future use as .zip file \n# #!zip -r vdblargec.zip /kaggle/working/vectordb\n\n#load chroma db instead of constructing it again (faster)\nvectordb = Chroma(persist_directory=\"/kaggle/input/vectordb/kaggle/working/vectordb\", embedding_function=embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Context Retrieval and Prompt Engineering <a name=\"Engineer\"></a>\n\nThe knowledge base in the RAG system enables the model to retrieve additional context to a question before answering the query. When a question is asked, chunks of text most similar to the query are retrieved and incorporated into a [prompt template](https://medium.com/@princekrampah/prompt-templates-in-langchain-248c015be3e0). Prompt templates are reproducible, consistent ways to ask a model questions, and they can be designed to reflect the desired style of responses. Below, the additional context for each question in the datasets is retrieved, and the prompt template encourages the model to use the context to aid in answering each question.","metadata":{}},{"cell_type":"code","source":"#Now that we have an embedding model, need to embed all of the queries in the training and testing datasets\nall_context=[]\n\nfor row in range(df.shape[0]):\n    #for memory purposes, grab only top relevant chunk\n    all_context.append(vectordb.similarity_search_with_relevance_scores(df['<QUESTION>'][row],k=1)[0][0].page_content)\n#create a new column in the dataset\ndf['<CONTEXT>']=all_context","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Separate the ground-truth response labels from the dataset\nX = df.drop(columns=['<RESPONSE>'])\ny = df['<RESPONSE>']\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=400, random_state=1)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=600, random_state=100)\n\n#Add context to training and testing sets \nX_train_with_RAG = list('<CONTEXT>:\\n' + X_train['<CONTEXT>'] + '\\n\\n<QUESTION>: Answer this <QUESTION> using the <CONTEXT> from above.\\n\\n'+ X_train['<QUESTION>'] + '\\n\\n<RESPONSE>:\\n'+y_train)\nX_test_with_RAG = list('<CONTEXT>:\\n' + X_test['<CONTEXT>'] + '\\n\\n<QUESTION>: Answer this <QUESTION> using the <CONTEXT> from above.\\n\\n'+ X_test['<QUESTION>'] + '\\n\\n<RESPONSE>:\\n')\nX_val_with_RAG = list('<CONTEXT>:\\n' + X_val['<CONTEXT>'] + '\\n\\n<QUESTION>: Answer this <QUESTION> using the <CONTEXT> from above.\\n\\n'+ X_val['<QUESTION>'] + '\\n\\n<RESPONSE>:\\n')\ny_test = y_test.reset_index()\ny_test = y_test['<RESPONSE>']\ny_val = y_val.reset_index()\ny_val = y_val['<RESPONSE>']\n\n#to measure if RAG helps on its own, get same training set but without context from RAG\nX_test_pre_RAG = X_test['<QUESTION>'].to_list()\ny_test_list = y_test.tolist()\nfor idx in range(len(X_test_pre_RAG)):\n    X_test_pre_RAG[idx] = \"<QUESTION>:\\n\\n\" + X_test_pre_RAG[idx] + \"\\n\\n<RESPONSE>:\\n\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Creation and Enabling Low Rank Adaptation (LoRA)<a name=\"Model\"></a>\n\nFine-tuning large language models can require massive amounts of computational resources. To overcome this challenge while still providing quality outputs, many modellers have used [Low Rank Adaptation (LoRA)](https://towardsdatascience.com/understanding-lora-low-rank-adaptation-for-finetuning-large-models-936bce1a07c6). Rather than update all weights in the model, the LoRA framework involves approximating the weight matrix updates using the product of low rank matrices. This effectively lowers the number of trainable parameters in the model.","metadata":{}},{"cell_type":"code","source":"#Load the Gemma model with specified hyperparameters\n\n#set seed for reproducability\ntf.random.set_seed(100)\n\n#load the instruct version of Gemma\nmodel = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_instruct_2b_en\")\n#rank 16 LoRA\nmodel.backbone.enable_lora(rank=16) \n#Only allow model to look back 305 characters for memory purposes\nmodel.preprocessor.sequence_length = 375\n#define the weight decay / learning-rate for the optimizer (AdamW is used for this notebook)\noptimizer = keras.optimizers.AdamW(\n    learning_rate=8.5e-4,\n    weight_decay=0.0005,\n    )\n\n#Exclude layernorm and bias terms from decay to speed up training and reduce memory consumption\noptimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n\n#compile the model by specifying the loss and other metrics we may want to track\nmodel.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=optimizer,\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation Metrics before Fine-Tuning<a name=\"Metrics\"></a>\n\nIt is imperative that some metric beyond human evaluation on a very small subset of responses is used to assess a model's response quality. This notebook includes the use of [ROUGE score](https://thepythoncode.com/article/calculate-rouge-score-in-python#rouge-score-a-premier-evaluation-metric). While often used in tasks beyond chatbot-related problems (such as text summarization), ROUGE score is one way to measure the similarity between generated responses to queries and the corresponding ground-truth labels. Three common variants are ROUGE-N, ROUGE-L, and ROUGE-S scores. \n\nSpecifically, this notebook includes the use of ROUGE-L and ROUGE-N (N=2) scores. ROUGE-L is based on the maximum common subsequence that appears in both the response and true label, helping to quantify sentence similariy rather than similarity based on strict word-to-word matches. ROUGE-2 scores quantify the overalp of bigrams between the response and true label.\n\nPut plainly, a ROUGE-2 recall of 0.3 means that 30% of the bigrams in the true label appear in the generated response. A ROUGE-2 precision of 0.3 means that 30% of the bigrams in the generated response appear in the true label. F-measure is a weighted mean of the precision and recall. F-measure is always between 0 and 1, and is only high if both precision and recall are high. ","metadata":{}},{"cell_type":"code","source":"def generate_test_set_responses(features_test):\n    '''\n    Get model's answers for queries listed in the test set for evaluation metrics\n\n    Parameters:\n    features_test (list): list of prompts to ask model\n\n    Returns:\n    responses (list): list of responses from model\n    '''\n    responses=[]\n    #for each prompt in the list\n    for idx in range(len(features_test)):\n    #generate the response\n        responses.append(model.generate(features_test[idx], max_length=512))\n    #update user\n        if(idx%100==0):\n            print(\"Finished number {}\".format(idx))\n            \n    #We don't need to return the question that is kept when calling model.generate()\n    keyword_length = len(\"<RESPONSE>:\\n\")\n    #for each of the responses\n    for idx in range(len(responses)):\n    #find where response begins, and start after \"<RESPONSE>:\\n\" substring\n        try:\n            start_idx = responses[idx].index(\"<RESPONSE>:\")\n            responses[idx]=responses[idx][(start_idx+keyword_length):]       \n        except:\n            pass\n            \n    return responses","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_rogue_metrics(y_hat,y_true,rouge_type):\n    '''\n    Calculate rogue score recall, precision, and f-measure for a given list \n    of model responses and ground-truth labels\n\n    Parameters:\n    y_hat (list): list of generated responses from model\n    y_true (list): list of ground-truth response labels\n    rouge_type (str): one of 'rougeL' or 'rougeN' where N is an integer\n\n    Returns:\n    all_recall (list): rogue score recall for each response, ground-truth label pair\n    all_precision (list): rogue score precision for each response, ground-truth label pair\n    all_f_measure (list): rogue score f-measure for each response, ground-truth label pair\n    '''\n    scorer = rouge_scorer.RougeScorer([rouge_type])\n    all_recall = [scorer.score(y_hat[idx],y_true[idx])[rouge_type].recall for idx in range(len(y_hat))]\n    all_precision = [scorer.score(y_hat[idx],y_true[idx])[rouge_type].precision for idx in range(len(y_hat))]\n    all_f_measure = [scorer.score(y_hat[idx],y_true[idx])[rouge_type].fmeasure for idx in range(len(y_hat))]\n    \n    return all_recall, all_precision, all_f_measure","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#assess the performance without RAG and without fine-tuning\ny_hat_pre_RAG = generate_test_set_responses(X_test_pre_RAG)\nrecall_pre, precision_pre, f_measure_pre = get_rogue_metrics(y_hat_pre_RAG, y_test_list,'rougeL')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Mean rogueL recall pre-RAG is {}\".format(round(np.mean(recall_pre),3)))\nprint(\"Mean rogueL precision pre-RAG is {}\".format(round(np.mean(precision_pre),3)))\nprint(\"Mean rogueL fmeasure pre-RAG is {}\".format(round(np.mean(f_measure_pre),3)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#assess the performance with RAG but without fine-tuning\ny_hat_post_RAG = generate_test_set_responses(X_test_with_RAG)\nrecall_post, precision_post, f_measure_post = get_rogue_metrics(y_hat_post_RAG, y_test_list,'rougeL')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Mean rogueL recall post-RAG is {}\".format(round(np.mean(recall_post),3)))\nprint(\"Mean rogueL precision post-RAG is {}\".format(round(np.mean(precision_post),3)))\nprint(\"Mean rogueL fmeasure post-RAG is {}\".format(round(np.mean(f_measure_post),3)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Fine-Tuning and Re-Evaluation <a name=\"FT\"></a>\n\nIt is common to [fine-tune](https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-models) pre-trained language models to achieve better performance at specific tasks. Widely speaking, while pre-trained models can learn general semantic understandings, exposing the model to information regarding specific domains can boost performance in that domain. Moreover, fine-tuning can aid in shaping the style of responses for the model. Below, the Gemma model is fine-tuned on data that was not incorporated into the RAG system. One would intuitively expect the corresponding ROUGE scores on held-out testing data to increase since the model is both exposed to more information about Python and exposed to the style of responses common from these datasets (assuming the held-out data has a similar \"distribution\" to the training data). However, such ROUGE scores can still act as an auditing tool to ensure model quality has not been worsened from an error in the codebase or a collapse in model training.\n\nBelow, an [adaptive learning rate](https://d2l.ai/chapter_optimization/lr-scheduler.html) is employed via a scheduler to hopefully achieve better optimization results. Additionally, the training is performed with [early stopping](https://machinelearningmastery.com/early-stopping-to-avoid-overtraining-neural-network-models/), meaning the algorithm will terminate if a validation metric becomes worse over time. Note that the early stopping method below is fairly conservative; rather than stop training immediately when the validation metric gets worse, it allows for (sometimes long) fluctuations. Essentially, then, the early stopping implementation below simply acts as a safeguard against an unexpected and significant plummeting of model performance. ","metadata":{}},{"cell_type":"code","source":"def ask_query(query):\n    '''\n    Given a query, gather appropriate context and pass both into model\n    to get a response\n\n    Parameters:\n    query (str): question to ask the model\n\n    Returns:\n    return_str (str): response from the model\n    '''\n    #for memory purposes, our model does not look back too many characters.\n    #therefore, just grab top 1 chunk most relevant to query\n    context = vectordb.similarity_search_with_relevance_scores(query,k=1)[0]\n    \n    #prompt engineering: add the conext to the question to make the prompt\n    entire_query = '<CONTEXT>:\\n' + context[0].page_content + '\\n\\n<QUESTION>: Answer this <QUESTION> using the <CONTEXT> from above.\\n\\n'+ query + '\\n\\n<RESPONSE>:\\n'\n    #feed into model\n    return_str = model.generate(entire_query,max_length=512)\n    #if there is a relevant document, tell the user where the context was located\n    if context[1] > .7:\n        return_str += '\\n\\nContext taken from '+ context[0].metadata['source'].rsplit('/', 1)[-1]\n    else:\n    #if no relevant documents, still keep in prompt but warn the user about this\n        return_str += '\\n\\n Warning: No relevant context for question in database.'\n    \n    #only return the actual response (not the question and context too)\n    return_str = return_str.rsplit('\\n\\n<RESPONSE>:\\n', 1)[-1]\n    return(return_str)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train the model\n\n#use early stopping to try and prevent overfitting\nearly_stopping = EarlyStopping(monitor='val_sparse_categorical_accuracy', patience=4, restore_best_weights=False, mode = \"max\",start_from_epoch=2)\n\n# Define the learning rate scheduler\nreduce_lr = ReduceLROnPlateau(monitor='sparse_categorical_accuracy', mode = \"max\", factor=0.05, patience=1, min_lr=1e-6,threshold=.02,threshold_mode='rel')\n\nhistory = model.fit(X_train_with_RAG, epochs=3, batch_size=1, callbacks=[reduce_lr,early_stopping],validation_data=(X_val_with_RAG, y_val))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#if we trained the model, save progress by running line below\n#model.backbone.save_lora_weights(\"/kaggle/working/PyGEM_lora_weights_all_saved.lora.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#if we are loading a model that is already fine-tuned, run the below line\nmodel.backbone.load_lora_weights(\"/kaggle/input/fine-tuned-pygem-weights/PyGEM_lora_weights_all_saved.lora.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#assess the performance with RAG and with fine-tuning\ny_hat = generate_test_set_responses(X_test_with_RAG)   \nrecall_ft, precision_ft, f_measure_ft = get_rogue_metrics(y_hat, y_test_list,'rougeL')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Mean rogueL recall post-Fine-Tuning is {}\".format(round(np.mean(recall_ft),3)))\nprint(\"Mean rogueL precision post-Fine-Tuning is {}\".format(round(np.mean(precision_ft),3)))\nprint(\"Mean rogueL fmeasure post-Fine-Tuning is {}\".format(round(np.mean(f_measure_ft),3)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ROUGE-2 Scores\nrecall_pre_2, precision_pre_2, f_measure_pre_2 = get_rogue_metrics(y_hat_pre_RAG, y_test_list,'rouge2')\nrecall_post_2, precision_post_2, f_measure_post_2 = get_rogue_metrics(y_hat_post_RAG, y_test_list,'rouge2')\nrecall_ft_2, precision_ft_2, f_measure_ft_2 = get_rogue_metrics(y_hat, y_test_list,'rouge2')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot change in metrics after incorporating RAG and after fine-tuning\ncategories = ['Base Model', 'With RAG', 'With RAG & Fine-Tuning']\nbars1 = [np.mean(f_measure_pre), np.mean(f_measure_post), np.mean(f_measure_ft)]\nbars2 = [np.mean(f_measure_pre_2), np.mean(f_measure_post_2), np.mean(f_measure_ft_2)] \n\n\nr1 = np.arange(len(categories))\nr2 = [x + 0.25 for x in r1]  # Adding 0.25 to each value in r1 to create space between bars\nplt.bar(r1, bars1, color='#6c8ebf', width=0.25, edgecolor='grey', label='Mean ROUGE-L f-measure')\nplt.bar(r2, bars2, color='#228B22', width=0.25, edgecolor='grey', label='Mean ROUGE-2 f-measure')\nplt.xticks([r + 0.125 for r in range(len(categories))], categories)\n\nplt.legend()\nplt.title(\"Metrics on Unseen Test Data\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Interactive Messaging and Displays <a name=\"UI\"></a>\n\nThe [ipywidgets package](https://ipywidgets.readthedocs.io/en/stable/) can be used to create a user-friendly and appealing way to interact with models like PyGEM. This package enables one to create interactive \"widgets\" that can trigger functions to be called when an action (such as clicking a button) is performed. An alternative to this method is to build a [Dash app](https://dash.plotly.com/) and host a dashboard on an external server. Here, the ipywidgets package is used for simplicity.","metadata":{}},{"cell_type":"code","source":"# Define custom CSS style/preferences below\n\ncustom_css = \"\"\"\n/* Style for output area */\n.output-area {\n    margin-top: 20px;\n    padding: 10px;\n    background-color: #f0f0f0;\n}\n\n/* Style for loading spinner */\n.spinner {\n    border: 3px solid #f3f3f3; /* Light grey */\n    border-top: 3px solid #3498db; /* Blue */\n    border-radius: 50%;\n    width: 30px; /* Adjust width */\n    height: 30px; /* Adjust height */\n    animation: spin 1s linear infinite; /* Apply animation */\n    margin: auto;\n    margin-top: 20px;\n}\n\n/* Keyframe animation for spinning */\n@keyframes spin {\n    0% { transform: rotate(0deg); }\n    100% { transform: rotate(360deg); }\n}\n\"\"\"\n\n# Apply custom CSS styles\ndisplay(HTML(\"<style>\" + custom_css + \"</style>\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ipywidgets\n#Create text box so user can ask question\ntext_box = widgets.Text(\n    value='',\n    placeholder='Ask PyGEM your question here',\n    description='Question:',\n    disabled=False,\n    layout=widgets.Layout(width='500px'),  \n    style={'description_width': 'initial'}\n)\n\n#Create \"Ask\" button to submit query to model\nsubmit_button = widgets.Button(\n    description='Ask',\n    disabled=False,\n    button_style='success',\n    tooltip='Ask'\n)\n\noutput_area = widgets.Output()\n\n#Define response function\ndef respond_to_question(sender):\n    question = text_box.value\n    \n    text_box.value = ''\n    \n    #Display loading spinner while model is generating a response\n    with output_area:\n        output_area.clear_output()\n        display(HTML('<div class=\"spinner\"></div>'))\n    \n    #Generate the model's response\n    response = ask_query(question)\n    \n    #Display the model's response to the user\n    with output_area:\n        output_area.clear_output()\n        print(f\"Question: {question}\\n\")\n        print(f\"Response: {response}\\n\")\n        \n#Assign response function to button click event\nsubmit_button.on_click(respond_to_question)\n\n#Define PyGEM logo\nimage_path = '/kaggle/input/pygem-logo/logo1.png'\nimage_widget = widgets.Image(value=open(image_path, 'rb').read(), format='png', width=400)\n\n#Define PyGEM's intro\ntext_widget = widgets.HTML(\n    value=\"<h2> <span style='margin-left: 100px;'>Hi! My name is PyGEM.<br><br> I was trained using Google's Gemma models to<br> answer any questions you have about the Python<br> programming language. Ask away!</h2>\",\n    layout=widgets.Layout(margin='25px 0') \n)\n\n#Display everything\ncontainer = widgets.VBox([image_widget,text_widget,widgets.HBox([text_box, submit_button]), output_area],)\ndisplay(container)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusions <a name=\"Conclusion\"></a>\n\n1. **Quantifiably increased performance**: By incorporating RAG into the pre-trained Gemma model, the average ROGUE-L f-measure score for unseen data increased from 0.194 to 0.224. Moreover, by also performing fine-tuning with existing and novel datasets, the f-measure score further increased to around 0.4. Similarly, the ROUGE-2 scores followed the same trajectory. With these concrete, quantifiable metrics combined with human evaluation on a few prompts, the increased performance and usability of the PyGEM model becomes clear. \n2. **Interpretability and Interactive Usage**: Via the RAG system, context from source documents is fed to the model along with the given prompt to form a response. The RAG system details which document contained the given context (and also warns the user if no relevant chunk of text is found), increasing interpretability. Moreover, the PyGEM model can be employed with a more user-friendly interface thanks to ipywidgets.\n3. **Computational Ease**: Due to the lightweight nature of Gemma models, the PyGEM model was easily created with limited computational resources. Fine-tuning the model via ten epochs with a dataset of around 2000 prompts on a single P100 GPU took less than 3 hours.\n\n# Limitations\n1. **Context Window**: Due to memory allocation concerns, the pre-processor for PyGEM has maximum sequence length of 375. This results in the model being limited in how far back it can \"look\" in the conversation/prompt before providing an answer. Consequently, the chunks for RAG needed to remain relatively small as well (potentially lowering the quality of the retrieved chunks).\n2. **Coding vs. Conceptual**: A primary goal for PyGEM was its versatility in being able to answer coding questions as well as general conceptual questions about Python. If one desired to build a model to solely aid in coding questions, for example, the training dataset would be more curated for this specific purpose.\n3. **Heuristic for RAG Warning**: There is no exact, systematic method for picking a similarity threshold in RAG for when to warn a user if a relevant document is not found. The threshold can be chosen according to how high-stakes the application is or based off of an empirical distribution of similarities gathered via model deployment.\n","metadata":{}},{"cell_type":"markdown","source":"### To go straight to the final model with RAG and the function to ask the model questions (without interface, explanation of code, or calculation of evaluation metrics as done below), can run the cell below <a name=\"MVP\"></a>","metadata":{}},{"cell_type":"code","source":"###########LOAD DEPENDENCIES###########\n!pip install langchain langchain-openai faiss-cpu tiktoken\n!pip install chromadb\n!pip install sentence-transformers\n\nimport numpy as np\nimport pandas as pd\nfrom langchain.vectorstores.chroma import Chroma\nimport keras\nimport keras_nlp\nimport tensorflow as tf\nfrom langchain.embeddings import SentenceTransformerEmbeddings\nimport sentencepiece\n\n###########GET EMBEDDING MODEL FOR RAG###########\nembeddings = SentenceTransformerEmbeddings(model_name = \"avsolatorio/GIST-small-Embedding-v0\")\n\n###########LOAD RAG DATABASE###########\nvectordb = Chroma(persist_directory=\"/kaggle/input/vectordb/kaggle/working/vectordb\", embedding_function=embeddings)\n\n###########LOAD GEMMA MODEL###########\nmodel = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_instruct_2b_en\")\n#rank 16 LoRA\nmodel.backbone.enable_lora(rank=16) \n#Only allow model to look back 300 characters for memory purposes\nmodel.preprocessor.sequence_length = 375\n#define the weight decay / learning-rate for the optimizer (AdamW is used for this notebook)\noptimizer = keras.optimizers.AdamW(\n    learning_rate=8.5e-4,\n    weight_decay=0.00025,\n    )\n\n#Exclude layernorm and bias terms from decay to speed up training and reduce memory consumption\noptimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n\n#compile the model by specifying the loss and other metrics we may want to track\nmodel.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=optimizer,\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\n\n\n###########LOAD LORA WEIGHTS FROM PREVIOUS TRAINING###########\nmodel.backbone.load_lora_weights(\"/kaggle/input/fine-tuned-pygem-weights/PyGEM_lora_weights_all_saved.lora.h5\")\n\n###########DEFINE FUNCTION TO ASK MODEL QUESTIONS###########\ndef ask_query(query):\n    '''\n    Given a query, gather appropriate context and pass both into model\n    to get a response\n\n    Parameters:\n    query (str): question to ask the model\n\n    Returns:\n    return_str (str): response from the model\n    '''\n    #for memory purposes, our model does not look back too many characters.\n    #therefore, just grab top 1 chunk most relevant to query\n    context = vectordb.similarity_search_with_relevance_scores(query,k=1)[0]\n    \n    #prompt engineering: add the conext to the question to make the prompt\n    entire_query = '<CONTEXT>:\\n' + context[0].page_content + '\\n\\n<QUESTION>: Answer this <QUESTION> using the <CONTEXT> from above.\\n\\n'+ query + '\\n\\n<RESPONSE>:\\n'\n    #feed into model\n    return_str = model.generate(entire_query,max_length=512)\n    #if there is a relevant document, tell the user where the context was located\n    if context[1] > .7:\n        return_str += '\\n\\nContext taken from '+ context[0].metadata['source'].rsplit('/', 1)[-1]\n    else:\n    #if no relevant documents, still keep in prompt but warn the user about this\n        return_str += '\\n\\n Warning: No relevant context for question in database.'\n    \n    #only return the actual response (not the question and context too)\n    return_str = return_str.rsplit('\\n\\n<RESPONSE>:\\n', 1)[-1]\n    return(return_str)\n\n\n###########ASK QUESTION WITHOUT USER INTERFACE###########\n\nask_query(\"What is Python?\")\n#add other questions here if desired","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Screenshots of Final Results <a name=\"results\"></a>\n\n## Example 1\n![](https://imgur.com/j4qjLY6.png)\n## Example 2\n![](https://imgur.com/RbxvJ4d.png)\n## Example 3\n![](https://imgur.com/MuYcwts.png)\n## Numerical Results\n![](https://imgur.com/XAXlzbT.png)","metadata":{}}]}